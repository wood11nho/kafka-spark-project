# Real-Time Transaction Monitoring with Kafka & Spark

Acest proiect demonstreazÄƒ cum putem construi un pipeline Big Data complet pentru detectarea tranzacÈ›iilor financiare mari Ã®n timp real, folosind **Apache Kafka** È™i **Apache Spark (Structured Streaming + SQL)**.

Scenariul simulat: un sistem bancar care monitorizeazÄƒ Ã®n timp real tranzacÈ›iile clienÈ›ilor pentru a semnala activitatea suspectÄƒ.

---

## âš™ï¸ Tehnologii utilizate

- **Apache Kafka** â€“ ingestia datelor (streaming)
- **Apache Spark** â€“ procesare È™i analizÄƒ Ã®n timp real
- **Spark SQL** â€“ filtrare È™i interogare a datelor
- **Python** â€“ dezvoltarea aplicaÈ›iilor
- **Docker Compose** â€“ orchestrare localÄƒ
- **Jupyter Notebook + Matplotlib** â€“ vizualizare output

---

## ğŸ§© Componente principale

- `producer.py` â€“ trimite mesaje JSON Ã®n Kafka (simuleazÄƒ tranzacÈ›ii aleatorii)
- `spark_app.py` â€“ consumÄƒ datele, filtreazÄƒ tranzacÈ›iile mari (>500) È™i alerteazÄƒ cele >900
- `docker-compose.yml` â€“ porneÈ™te Kafka, Zookeeper È™i Spark Ã®n containere
- `visualize.ipynb` â€“ Ã®ncarcÄƒ fiÈ™iere Parquet È™i genereazÄƒ o histogramÄƒ a tranzacÈ›iilor

---

## ğŸš€ Cum rulezi proiectul

1. **PorneÈ™te mediul de lucru**
   ```bash
   docker-compose up -d
   ```

2. **RuleazÄƒ generatorul de tranzacÈ›ii**
   ```bash
   python producer.py
   ```

3. **ConecteazÄƒ-te la containerul Spark È™i ruleazÄƒ aplicaÈ›ia**
   ```bash
   docker exec -it <nume_container_spark> bash
   spark-submit spark_app.py
   ```

4. **(OpÈ›ional) ActiveazÄƒ scrierea Ã®n Parquet Ã®n `spark_app.py` pentru a salva datele**
   > Ãn fiÈ™ierul `spark_app.py`, decomenteazÄƒ secÈ›iunea `df_fraude.writeStream...`

5. **VizualizeazÄƒ rezultatele Ã®n Jupyter**
   ```bash
   jupyter notebook
   ```
   RuleazÄƒ `visualize.ipynb` pentru a genera o histogramÄƒ a tranzacÈ›iilor mari.

---

## ğŸ“Œ Logica de business

- TranzacÈ›iile sunt generate cu sume Ã®ntre 1 È™i 1000.
- Cele >500 RON sunt considerate "mari".
- Cele >900 RON declanÈ™eazÄƒ **alertÄƒ Ã®n consolÄƒ**.
- Datele pot fi salvate Ã®n format Parquet pentru analizÄƒ ulterioarÄƒ.

---

## ğŸ“ˆ Exemplu de vizualizare

Graficul generat evidenÈ›iazÄƒ distribuÈ›ia valorilor tranzacÈ›ionate peste 500 RON â€” util pentru analiza anomaliilor sau pragurilor critice.

---

## ğŸ§  ObservaÈ›ii finale

- Proiectul aratÄƒ cum Kafka È™i Spark pot colabora pentru un sistem de streaming scalabil.
- Extensii posibile: alertare pe email, stocare Ã®n baze de date, integrare cu dashboard-uri live (Grafana).
- Codul este modular È™i poate fi adaptat uÈ™or la orice alt domeniu unde e nevoie de procesare Ã®n timp real.

---

ğŸ“ *Proiect realizat Ã®n cadrul cursului Big Data â€“ Master NLP, Universitatea din BucureÈ™ti, 2025.*

